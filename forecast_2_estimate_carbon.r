# This script reads the k.parquet and matches.parquet files generated by the implementatio code for each project,
# retrieves and saves as csv files the following three time series for 100 subsampled pixel sets of each project:
# 1. Ex post additionality (additionality_[proj].csv)
# 2. Historical project carbon loss rate (project_closs_rate_[proj].csv)
# 3. Historical regional carbon loss rates (regional_closs_rate_[proj].csv)
# It then calculates and saves as csv files the bootstrapped mean over the 100 sets of each project:
# 1. Ex post additionality (boot_additionality_[proj].csv)
# 2. Historical project carbon loss rate (boot_project_closs_rate_[proj].csv)
# 3. Historical regional carbon loss rates (boot_regional_closs_rate_[proj].csv)

# The additionality_[proj].csv files contain the following columns:
# year: year
# pair: subsample ID
# counterfactual: carbon density in the counterfactual scenario at the given year (MgC ha-1)
# project: carbon density in the project area at the given year (MgC ha-1)
# diff_cf: counterfactual carbon loss from t0 to the given year (MgC ha-1)
# diff_p: project carbon loss from t0 to the given year (MgC ha-1)
# additionality_whole: carbon credits (additionality) from t0 to the given year (MgC ha-1 yr-1)
# additionality_annual: annual carbon credit generation rate (annual additionality) from t0 to the given year (MgC ha-1 yr-1)

# The project/regional_closs_rate_[proj].csv files contain the following columns:
# year: year
# pair: subsample ID
# counterfactual: carbon density in the counterfactual scenario at the given year (MgC ha-1)
# project: carbon density in the project area at the given year (MgC ha-1)
# closs: counterfactual carbon loss from the given year to t0 (MgC ha-1)
# closs_annual: annual compound carbon loss rate from the given year to t0 (MgC ha-1 yr-1)

# It requires the following input variables:
#1. parquet_path: absolute path of the directory containing implementation code output (k.parquet, matches.parquet) of each project
#2. out_path: absolute path (plus prefix) of files generated by this repo, including in forecast_1_setup.r

rm(list = ls())

#Load packages
library(tidyverse) #ggplot2, dplyr, and stringr used in plotPlacebo/plotBaseline.r: tibble to store labels with bquote()
library(magrittr) #pipe operators
library(future) #parallelise lapply() : future_lapply()
library(future.apply) #parallelise lapply(): future_lapply()
library(sf) #st_drop_geometry() used in GetCarbonLoss.r (runs on GDAL 3.10)
library(arrow) #read_parquet()
library(MatchIt) #matchit(), used in the customised function AssessBalance()
library(data.table) #setDT: convert setM to data table to speed up sub-sampling
library(boot) #boot::boot

#Set parallelise plan
plan(multisession, workers = 25)

#Load pre-defined functions
source("FindFiles.r") #wrapper function to search files or folders based on inclusion/exclusion keywords
source("ReformatPixels.r") #wrapper function to reformat column names of data frame of matched pixels
source("AssessBalance.r") #function to assess matching balance
source("GetCarbonLoss.r") #function to generate time series of annual change in project-level average carbon density
source("BootOut.r")

#Define input variables
parquet_path = "/maps/epr26/tmf_pipe_out/" #path to directories containing implementation outputs
out_path = "/maps/epr26/ex_ante_forecast_out/out_ongoing" #path of files generated by this repo, including in forecast_1_setup.r

project_var = read.csv(paste0(out_path, "_project_var.csv"), header = T)

projects = project_var$ID
t0_vec = project_var$t0
area_ha_vec = project_var$area_ha
cdens_list = project_var %>%
  dplyr::select(ID, cdens_1:cdens_6) %>%
  pivot_longer(cdens_1:cdens_6, names_to = "land.use.class", names_prefix = "cdens_", values_to = "carbon.density") %>%
  split(f = .$ID)

#Retrieve input paths needed for the analysis
parquet_dirs = paste0(parquet_path, projects)
k_paths = rep(NA, length(projects))
m_paths = rep(NA, length(projects))

#retrieve project-level environmental characteristics
var_envir_list = vector("list", length(projects))
for(i in seq_along(projects)) {
  k_paths[i] = FindFiles(parquet_dirs[i], "k.parquet", full = T)
  m_paths[i] = FindFiles(parquet_dirs[i], "matches.parquet", full = T)

  setK = read_parquet(k_paths[i])
  setM = read_parquet(m_paths[i])
  var_envir_list[[i]] = data.frame(prj_elev = mean(setK$elevation, na.rm = T),
                                   prj_slope = mean(setK$slope, na.rm = T),
                                   prj_remote = mean(setK$access, na.rm = T),
                                   reg_elev = mean(setM$elevation, na.rm = T),
                                   reg_slope = mean(setM$slope, na.rm = T),
                                   reg_remote = mean(setM$access, na.rm = T))
}
var_envir = list_rbind(var_envir_list)
project_var_envir = cbind(project_var, var_envir)

#load socio-economic variables for each country
#GDP per capita (US$) in 2023
gdppc = read.csv("/maps/epr26/API_NY.GDP.PCAP.CD_DS2_en_csv_v2_85121.csv", header = T, skip = 4) %>%
  dplyr::select(Country.Name, X2023) %>%
  rename(country = Country.Name, gdppc_2023 = X2023)

#World Governance Index (Control of Corruption, cc) in 2023
wgi = read.csv("/maps/epr26/wgidataset.csv", header = T) %>%
  filter(indicator == "cc" & year == 2023) %>%
  dplyr::select(countryname, estimate, stddev) %>%
  rename(country = countryname, wgicc_estimate = estimate, wgicc_stddev = stddev)

project_var_gdppc = merge(project_var_envir, gdppc, by = "country", all.x = T)
project_var_complete = merge(project_var_gdppc, wgi, by = "country", all.x = T) %>%
  arrange(code)
write.csv(project_var_complete, paste0(out_path, "_project_var_complete.csv"), row.names = F)


#retrieve carbon time series for project pixel subsamples and matched pixels
for(i in seq_along(projects)) {
  t0 = t0_vec[i]
  luc_t_20 = paste0("luc_", t0_vec[i] - 20)
  luc_t_10 = paste0("luc_", t0_vec[i] - 10)
  luc_t0 = paste0("luc_", t0_vec[i])
  area_ha = area_ha_vec[i]
  cdens = cdens_list[[i]]
  pair_dir = paste0(parquet_dirs[i], "/pairs/")

  a = Sys.time()
  #find paths to match and unmatached points in each sampled pairs
  pair_paths = FindFiles(pair_dir, include = ".parquet", full = T)
  matched_paths = pair_paths %>% str_subset("matchless", negate = T)
  matchless_paths = pair_paths %>% str_subset("matchless")

  if(length(matched_paths) == 0) {
    cat("No matches\n")
    closs_df = NULL
    expost_add = NULL
    closs_project = NULL
  } else {
    pairs_out = future_lapply(seq_along(matched_paths), function(j) {
      pair_start = Sys.time()

      matched_path = matched_paths[j]
      matchless_path = matchless_paths[j]

      pairs = read_parquet(matched_path) %>%
        dplyr::select(-dplyr::ends_with(c("_x", "_y", "_trt", "_cluster")))
      unmatched_pairs = read_parquet(matchless_path) %>%
        dplyr::select(-dplyr::ends_with(c("_x", "_y", "_trt", "_cluster")))

      #calculate proportion of unmatched pixels, used to adjust the area represented by sample: is this still necessary?
      area_adj_ratio = nrow(pairs) / (nrow(pairs) + nrow(unmatched_pairs))

      pixels_cf = ReformatPixels(in_df = pairs, prefix = "s_", t0 = t0, treatment = "counterfactual", pair = j)
      pixels_p = ReformatPixels(in_df = pairs, prefix = "k_", t0 = t0, treatment = "project", pair = j)
      pixels_matched = rbind(pixels_cf, pixels_p)

      #assess matching balance
      balance_assessment = AssessBalance(pixels_matched, t0 = t0)

      #retrieve carbon time series for project and matched counterfactual
      carbon_cf = GetCarbonLoss(pixels_cf, t0, area_ha, area_adj_ratio, cdens, pair = j)
      carbon_p = GetCarbonLoss(pixels_p, t0, area_ha, area_adj_ratio, cdens, pair = j)

      pair_end = Sys.time()
      cat(j, ":", format(difftime(pair_end, pair_start, units = "secs")), "\n")

      return(list(carbon_cf = carbon_cf, carbon_p = carbon_p,
                  pixels_matched = pixels_matched, area_adj_ratio = area_adj_ratio,
                  balance_assessment = balance_assessment))
    }, future.seed = T)
  }

  b = Sys.time()
  cat("Project", i, "/", length(projects), "-", projects[i], "- project carbon loss :", format(difftime(b, a, units = "secs")), "\n")

  #reformat into wide data frames of all projects and pairs
  carbon_cf = lapply(pairs_out, function(x) x$carbon_cf) %>%
    list_rbind() %>%
    mutate(treatment = "counterfactual")

  carbon_p = lapply(pairs_out, function(x) x$carbon_p) %>%
    list_rbind() %>%
    mutate(treatment = "project")

  carbon_matched = rbind(carbon_cf, carbon_p) %>%
    pivot_wider(values_from = "carbon_density", names_from = "treatment")
  tmax = max(carbon_matched$year)

  #calculate ex post annual counterfactual carbon loss rates and per-area carbon credit generation rates
  additionality = carbon_matched %>%
    filter(year >= t0) %>%
    group_by(pair) %>%
    mutate(diff_cf = first(counterfactual) - counterfactual,
           diff_p = first(project) - project,
           cf_closs = exp(log(diff_cf) / (year - t0)),
           additionality_whole = diff_cf - diff_p,
           additionality_annual = exp(log(additionality_whole) / (year - t0)))

  #calculate ex ante within-project annual per-area carbon loss rates
  ante_project = carbon_matched %>%
    filter(year <= t0) %>%
    group_by(pair) %>%
    mutate(closs = project - last(project),
           closs_annual = exp(log(closs) / (t0 - year)))

  write.csv(additionality, paste0(out_path, "_additionality_", projects[i], ".csv"), row.names = F)
  write.csv(ante_project, paste0(out_path, "_project_closs_rate_", projects[i], ".csv"), row.names = F)

  #retrieve carbon time series for surrounding region
  #convert to data table for faster subsampling and first down-sample to 250000
  setM = setDT(read_parquet(m_paths[i]))
  setM_subsamp = setM[sample(.N, 250000, replace = T)]

  #do 100 random 10% sub-samples to allow bootstrapping
  a = Sys.time()
  boot_out = future_lapply(1:100, function(j) {
    subsamp_start = Sys.time()
    setM_boot = setM_subsamp[sample(.N, 25000, replace = T)]
    pixels_region = ReformatPixels(in_df = setM_boot, prefix = "", t0 = t0, treatment = "region", pair = j)
    closs_region = GetCarbonLoss(pixels_region, t0, area_ha, area_adj_ratio = 1, cdens, pair = j)
    subsamp_end = Sys.time()
    cat(j, ":", format(difftime(subsamp_end, subsamp_start, units = "secs")), "\n")
    return(closs_region)
  }, future.seed = T)
  b = Sys.time()
  cat("Project", i, "/", length(projects), "-", projects[i], "- regional carbon loss :", format(difftime(b, a, units = "secs")), "\n")

  closs_region = boot_out %>%
    list_rbind() %>%
    mutate(treatment = "region")

  #calculate ex ante regional annual per-area carbon loss rates
  ante_region = closs_region %>%
    filter(year <= t0) %>%
    group_by(pair) %>%
    mutate(closs = carbon_density - last(carbon_density),
           closs_annual = ifelse(closs > 0, exp(log(closs) / (t0 - year)), NA))

  write.csv(ante_region, paste0(out_path, "_regional_closs_rate_", projects[i], ".csv"), row.names = F)
}


# C. Bootstrap outcomes ----
cf_closs_boot_list = vector("list", length(projects))
additionality_boot_list = vector("list", length(projects))
ante_project_boot_list = vector("list", length(projects))
ante_region_boot_list = vector("list", length(projects))

for(i in seq_along(projects)) {
  t0 = t0_vec[i]
  project_i = projects[i]
  additionality = read.csv(paste0(out_path, "_additionality_", projects[i], ".csv"), header = T)
  ante_project = read.csv(paste0(out_path, "_project_closs_rate_", projects[i], ".csv"), header = T)
  ante_region = read.csv(paste0(out_path, "_regional_closs_rate_", projects[i], ".csv"), header = T)
  tmax = max(additionality$year)

  #bootstrap ex post counterfactual carbon loss rate
  cf_closs_boot_list[[i]] = BootOut(in_df = additionality, column = "cf_closs", from = t0 + 1, to = tmax) %>%
    mutate(project = project_i)
  #bootstrap ex post additionality
  additionality_boot_list[[i]] = BootOut(in_df = additionality, column = "additionality_annual", from = t0 + 1, to = tmax) %>%
    mutate(project = project_i)

  #bootstrap ex ante project carbon loss rate
  ante_project_boot_list[[i]] = BootOut(in_df = ante_project, column = "closs_annual", from = t0 - 10, to = t0 - 1) %>%
    mutate(project = project_i)

  #bootstrap ex ante regional carbon loss rate
  ante_region_boot_list[[i]] = BootOut(in_df = ante_region, column = "closs_annual", from = t0 - 10, to = t0 - 1) %>%
    mutate(project = project_i)
}

cf_closs_boot_df = list_rbind(cf_closs_boot_list)
additionality_boot_df = list_rbind(additionality_boot_list)
ante_project_boot_df = list_rbind(ante_project_boot_list)
ante_region_boot_df = list_rbind(ante_region_boot_list)

write.csv(cf_closs_boot_df, paste0(out_path, "_boot_cf_closs.csv"), row.names = F)
write.csv(additionality_boot_df, paste0(out_path, "_boot_additionality.csv"), row.names = F)
write.csv(ante_project_boot_df, paste0(out_path, "_boot_project_closs_rate.csv"), row.names = F)
write.csv(ante_region_boot_df, paste0(out_path, "_boot_regional_closs_rate.csv"), row.names = F)