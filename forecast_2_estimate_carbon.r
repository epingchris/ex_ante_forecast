# This script reads the k.parquet and matches.parquet files generated by the implementatio code for each project,
# retrieves and saves as csv files the following three time series for 100 subsampled pixel sets of each project:
# 1. Ex post additionality (additionality_[proj].csv)
# 2. Historical project carbon loss rate (project_closs_rate_[proj].csv)
# 3. Historical regional carbon loss rates (regional_closs_rate_[proj].csv)
# It then calculates and saves as csv files the bootstrapped mean over the 100 sets of each project:
# 1. Ex post additionality (boot_additionality_[proj].csv)
# 2. Historical project carbon loss rate (boot_project_closs_rate_[proj].csv)
# 3. Historical regional carbon loss rates (boot_regional_closs_rate_[proj].csv)

# The additionality_[proj].csv files contain the following columns:
# year: year
# pair: subsample ID
# counterfactual: carbon density in the counterfactual scenario at the given year (MgC ha-1)
# project: carbon density in the project area at the given year (MgC ha-1)
# diff_cf: counterfactual carbon loss from t0 to the given year (MgC ha-1)
# diff_p: project carbon loss from t0 to the given year (MgC ha-1)
# additionality_whole: carbon credits (additionality) from t0 to the given year (MgC ha-1 yr-1)
# additionality_annual: annual carbon credit generation rate (annual additionality) from t0 to the given year (MgC ha-1 yr-1)

# The project/regional_closs_rate_[proj].csv files contain the following columns:
# year: year
# pair: subsample ID
# counterfactual: carbon density in the counterfactual scenario at the given year (MgC ha-1)
# project: carbon density in the project area at the given year (MgC ha-1)
# closs: counterfactual carbon loss from the given year to t0 (MgC ha-1)
# closs_annual: annual compound carbon loss rate from the given year to t0 (MgC ha-1 yr-1)

# It requires the following input variables:
#1. project_dir: absolute path of the directory containing implementation code output (k.parquet, matches.parquet) of each project
#2. out_path: absolute path (plus prefix) of files generated by this repo, including in forecast_1_setup.r

rm(list = ls())

#Load packages
library(tidyverse) #ggplot2, dplyr, and stringr used in plotPlacebo/plotBaseline.r: tibble to store labels with bquote()
library(magrittr) #pipe operators
library(parallel) #detectCores()
library(future) #parallelise lapply() : future_lapply()
library(future.apply) #parallelise lapply(): future_lapply()
library(sf) #st_drop_geometry() used in GetCarbonLoss.r (runs on GDAL 3.10)
library(arrow) #read_parquet()
library(MatchIt) #matchit(), used in the customised function AssessBalance()
library(data.table) #setDT: convert setM to data table to speed up sub-sampling
library(boot) #boot::boot

#Set optional user-selected project(s) to run
args = commandArgs(trailingOnly = T)
if (length(args) == 0) {
  #all projects in project_var.csv
  param_p = "all"
} else {
  #user-selected projects
  param_p = args
}

#Set parallelise plan
plan(multisession, workers = 20)

#Load pre-defined functions
source("FindFiles.r") #wrapper function to search files or folders based on inclusion/exclusion keywords
source("ReformatPixels.r") #wrapper function to reformat column names of data frame of matched pixels
source("AssessBalance.r") #function to assess matching balance
source("GetCarbonSeries.r") #function to generate time series of annual change in project-level average carbon density
source("BootOut.r")

#Define input variables
project_dir = "/maps/epr26/tmf_pipe_out/" #path to directories containing implementation outputs
out_path = "/maps/epr26/ex_ante_forecast_out/out_ongoing" #path of files generated by this repo, including in forecast_1_setup.r

project_var = read.csv(paste0(out_path, "_project_var.csv"), header = T)

if(param_p[1] == "all") {
  project_sel = project_var
} else {
  project_sel = project_var %>%
    filter(ID %in% param_p)
}


#Read project variables
projects = project_sel$ID
t0_vec = project_sel$t0
area_ha_vec = project_sel$area_ha
cdens_list = project_sel %>%
  dplyr::select(ID, cdens_1:se_6) %>%
  pivot_longer(cols = c(starts_with("cdens_"), starts_with("n_"), starts_with("se_")),
              names_to = c(".value", "luc"),
              names_sep = "_") %>%
  split(f = .$ID)


#retrieve carbon time series for project pixel subsamples and matched pixels
for(i in seq_along(projects)) {
  t0 = t0_vec[i]
  area_ha = area_ha_vec[i]
  cdens = cdens_list[[i]]
  cdens$se[is.na(cdens$se)] = 0  #if there is an NA in the se column, replace it with 0
  pair_dir = paste0(project_dir, projects[i], "/pairs/")

  a = Sys.time()
  #find paths to match and unmatached points in each sampled pairs
  pair_paths = FindFiles(pair_dir, include = ".parquet", full = T)
  matched_paths = pair_paths %>% str_subset("matchless", negate = T)
  matchless_paths = pair_paths %>% str_subset("matchless")

  if(length(matched_paths) == 0) {
    cat("No matches\n")
    closs_df = NULL
    expost_add = NULL
    closs_project = NULL
  } else {
    pairs_out = future_lapply(seq_along(matched_paths), function(j) {
      pair_start = Sys.time()

      matched_path = matched_paths[j]
      matchless_path = matchless_paths[j]

      pairs = read_parquet(matched_path) %>%
        dplyr::select(-dplyr::ends_with(c("_x", "_y", "_trt", "_cluster")))
      unmatched_pairs = read_parquet(matchless_path) %>%
        dplyr::select(-dplyr::ends_with(c("_x", "_y", "_trt", "_cluster")))

      #calculate proportion of unmatched pixels, used to adjust the area represented by sample: is this still necessary?
      area_adj_ratio = nrow(pairs) / (nrow(pairs) + nrow(unmatched_pairs))

      #reformat column names and split into project and counterfactual pixels
      pixels_p = ReformatPixels(in_df = pairs, prefix = "k_", t0 = t0, treatment = "project", pair = j)
      pixels_cf = ReformatPixels(in_df = pairs, prefix = "s_", t0 = t0, treatment = "counterfactual", pair = j)
      pixels_matched = rbind(pixels_p, pixels_cf)
      b = Sys.time()

      #assess matching balance; currently not used
      balance_assessment = AssessBalance(pixels_matched, t0 = t0)

      #generate bootstrapped samples of reference carbon density for each luc
      n_boot = 1000
      cdens_boot = cdens %>%
        rowwise() %>%
        mutate(boot = list(rnorm(n_boot, mean = cdens, sd = se))) %>%
        ungroup() %>%
        dplyr::select(boot) %>%
        unnest_wider(boot, names_sep = "_")
      carbon_p = vector("list", n_boot)
      carbon_cf = vector("list", n_boot)
      for(k in seq_len(n_boot)) {
        a = Sys.time()
        cdens_k = data.frame(luc = 1:6, cdens = cdens_boot[, i])
        colnames(cdens_k) = c("luc", "cdens")
        #retrieve carbon time series for project and matched counterfactual
        carbon_p[[k]] = GetCarbonSeries(pixels_p, t0, area_ha, area_adj_ratio, cdens = cdens_k) %>%
          mutate(cdens_boot = k)
        carbon_cf[[k]] = GetCarbonSeries(pixels_cf, t0, area_ha, area_adj_ratio, cdens = cdens_k) %>%
          mutate(cdens_boot = k)
        b = Sys.time()
        cat("Cdens boot", k, ":", format(difftime(b, a, units = "secs")), "\n")
      }
      carbon_p = list_rbind(carbon_p)
      carbon_cf = list_rbind(carbon_cf)

      pair_end = Sys.time()
      cat(j, ":", format(difftime(pair_end, pair_start, units = "secs")), "\n")

      return(list(carbon_p = carbon_p, carbon_cf = carbon_cf,
                  pixels_matched = pixels_matched, area_adj_ratio = area_adj_ratio,
                  balance_assessment = balance_assessment))
    }, future.seed = T)
  }

  b = Sys.time()
  cat("Project", i, "/", length(projects), "-", projects[i], "- project carbon loss :", format(difftime(b, a, units = "secs")), "\n")

  #reformat into wide data frames of all projects and pairs
  carbon_p = lapply(pairs_out, function(x) x$carbon_p) %>%
    list_rbind() %>%
    mutate(treatment = "project")

  carbon_cf = lapply(pairs_out, function(x) x$carbon_cf) %>%
    list_rbind() %>%
    mutate(treatment = "counterfactual")

  carbon_matched = rbind(carbon_cf, carbon_p) %>%
    pivot_wider(values_from = "carbon_density", names_from = "treatment")

  #calculate per-area annual carbon credit generation rates (additionality)
  additionality = carbon_matched %>%
    filter(year >= t0) %>%
    group_by(pair) %>%
    mutate(diff_cf = first(counterfactual) - counterfactual,
           diff_p = first(project) - project,
           additionality = diff_cf - diff_p,
           additionality_arith = additionality / (year - t0),
           additionality_geom = additionality ^ (1 / (year - t0)))

  #calculate ex post annual counterfactual carbon loss rates
  obs_counterfactual = carbon_matched %>%
    filter(year >= t0) %>%
    group_by(pair) %>%
    mutate(closs = 1 - (counterfactual / first(counterfactual)) ^ (1 / (year - t0)))

  #calculate historical within-project annual per-area carbon loss rates
  historical_project = carbon_matched %>%
    filter(year <= t0) %>%
    group_by(pair) %>%
    mutate(closs = 1 - (last(project) / project) ^ (1 / (t0 - year)))

  write.csv(additionality, paste0(out_path, "_additionality_", projects[i], ".csv"), row.names = F)
  write.csv(obs_counterfactual, paste0(out_path, "_closs_observed_", projects[i], ".csv"), row.names = F)
  write.csv(historical_project, paste0(out_path, "_closs_project_", projects[i], ".csv"), row.names = F)

  #retrieve carbon time series for surrounding region
  m_path = FindFiles(paste0(project_dir, projects[i]), "matches.parquet", full = T)
  setM = setDT(read_parquet(m_path)) #convert to data table for faster subsampling
  setM_subsamp = setM[sample(.N, 250000, replace = T)] #down-sample to 250000

  #do 100 random 10% sub-samples to allow bootstrapping
  a = Sys.time()
  region_boot = future_lapply(1:100, function(j) {
    subsamp_start = Sys.time()
    setM_boot = setM_subsamp[sample(.N, 25000, replace = T)]
    pixels_region = ReformatPixels(in_df = setM_boot, prefix = "", t0 = t0, treatment = "region", pair = j)

    #generate bootstrapped samples of reference carbon density for each luc
    n_boot = 1000
    cdens_boot = cdens %>%
      rowwise() %>%
      mutate(boot = list(rnorm(n_boot, mean = cdens, sd = se))) %>%
      ungroup() %>%
      dplyr::select(boot) %>%
      unnest_wider(boot, names_sep = "_")
    carbon_region = vector("list", n_boot)
    for(k in seq_len(n_boot)) {
      a = Sys.time()
      cdens_k = data.frame(luc = 1:6, cdens = cdens_boot[, i])
      colnames(cdens_k) = c("luc", "cdens")
      #retrieve carbon time series for project and matched counterfactual
      carbon_region[[k]] = GetCarbonSeries(pixels_region, t0, area_ha, area_adj_ratio = 1, cdens = cdens_k) %>%
        mutate(cdens_boot = k)
      b = Sys.time()
      cat("Cdens boot", k, ":", format(difftime(b, a, units = "secs")), "\n")
    }
    carbon_region = list_rbind(carbon_region)
    subsamp_end = Sys.time()
    cat(j, ":", format(difftime(subsamp_end, subsamp_start, units = "secs")), "\n")
    return(carbon_region)
  }, future.seed = T)
  b = Sys.time()
  cat("Project", i, "/", length(projects), "-", projects[i], "- regional carbon loss :", format(difftime(b, a, units = "secs")), "\n")

  #calculate historical regional annual per-area carbon loss rates
  historical_region = region_boot %>%
    list_rbind() %>%
    mutate(treatment = "region") %>%
    filter(year <= t0) %>%
    group_by(pair) %>%
    mutate(closs = 1 - (last(carbon_density) / carbon_density) ^ (1 / (t0 - year)))

  write.csv(historical_region, paste0(out_path, "_closs_regional_", projects[i], ".csv"), row.names = F)
}


# C. Bootstrap outcomes ----
boot_additionality_list = vector("list", length(projects))
boot_closs_observed_list = vector("list", length(projects))
boot_closs_project_list = vector("list", length(projects))
boot_closs_region_list = vector("list", length(projects))

for(i in seq_along(projects)) {
  a = Sys.time()
  t0 = t0_vec[i]
  project_i = projects[i]
  additionality = read.csv(paste0(out_path, "_additionality_", projects[i], ".csv"), header = T)
  closs_observed = read.csv(paste0(out_path, "_closs_observed_", projects[i], ".csv"), header = T)
  closs_project = read.csv(paste0(out_path, "_closs_project_", projects[i], ".csv"), header = T)
  closs_regional = read.csv(paste0(out_path, "_closs_regional_", projects[i], ".csv"), header = T)
  tmax = max(additionality$year)

  #bootstrap ex post additionality accumulation rate: use arithmetic mean of annual additionality
  boot_additionality_list[[i]] = BootOut(in_df = additionality, column = "additionality_arith", from = t0 + 1, to = tmax) %>%
    mutate(project = project_i)

  #bootstrap ex post counterfactual carbon loss rate
  boot_closs_observed_list[[i]] = BootOut(in_df = closs_observed, column = "closs", from = t0 + 1, to = tmax) %>%
    mutate(project = project_i)

  #bootstrap historical carbon loss rate
  boot_closs_project_list[[i]] = BootOut(in_df = closs_project, column = "closs", from = t0 - 10, to = t0 - 1) %>%
    mutate(project = project_i)

  #bootstrap historical regional carbon loss rate
  boot_closs_region_list[[i]] = BootOut(in_df = closs_regional, column = "closs", from = t0 - 10, to = t0 - 1) %>%
    mutate(project = project_i)
  b = Sys.time()
  cat("Project", i, "/", length(projects), "-", projects[i], "- bootstrapping :", format(difftime(b, a, units = "secs")), "\n")
}

boot_additionality_df = list_rbind(boot_additionality_list)
boot_closs_observed_df = list_rbind(boot_closs_observed_list)
boot_closs_project_df = list_rbind(boot_closs_project_list)
boot_closs_region_df = list_rbind(boot_closs_region_list)

write.csv(boot_additionality_df, paste0(out_path, "_boot_additionality.csv"), row.names = F)
write.csv(boot_closs_observed_df, paste0(out_path, "_boot_closs_observed.csv"), row.names = F)
write.csv(boot_closs_project_df, paste0(out_path, "_boot_closs_project.csv"), row.names = F)
write.csv(boot_closs_region_df, paste0(out_path, "_boot_closs_regional.csv"), row.names = F)